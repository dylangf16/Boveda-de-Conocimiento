---
Fecha de creaci贸n: <% tp.date.now("YYYY-MM-DD HH:mm") %>
Fecha de Modificaci贸n: <% tp.date.now("YYYY-MM-DD HH:mm") %>
tags: 
Tema: Large Language Models (LLMs)
---

##  Idea/Concepto 
Redes neuronales profundas basadas en decodificadores Transformer que aprenden auto-supervisadamente prediciendo la siguiente palabra en billones de tokens. Generan texto auto-regresivamente usando embeddings y atenci贸n para capturar significado y relaciones contextuales. "Large" por par谩metros, datos y c贸mputo requerido.

##  Puntos Claves
- Arquitectura: decodificadores Transformer
- Aprendizaje auto-supervisado en texto masivo
- Generaci贸n auto-regresiva token por token
- Post-entrenamiento: fine-tuning para tareas espec铆ficas
- Escala masiva en tres dimensiones: par谩metros, datos, c贸mputo

##  Connections
- [[Redes Neuronales]]
- [[Mecanismo de Atenci贸n]]
- [[Ventana Contextual]]
- [[Tokenizaci贸n]]
- [[Embeddings]]

##  Personal Insight
Los LLMs representan una emergencia de capacidades generales a partir del simple objetivo de predicci贸n de siguiente palabra, demostrando que la escala puede generar inteligencia.

## Ь Recursos
-