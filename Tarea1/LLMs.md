---
Fecha de creación: <% tp.date.now("YYYY-MM-DD HH:mm") %>
Fecha de Modificación: <% tp.date.now("YYYY-MM-DD HH:mm") %>
tags: 
Tema: Large Language Models (LLMs)
---

## 📚 Idea/Concepto 
Redes neuronales profundas basadas en decodificadores Transformer que aprenden auto-supervisadamente prediciendo la siguiente palabra en billones de tokens. Generan texto auto-regresivamente usando embeddings y atención para capturar significado y relaciones contextuales. "Large" por parámetros, datos y cómputo requerido.

## 📌 Puntos Claves
- Arquitectura: decodificadores Transformer
- Aprendizaje auto-supervisado en texto masivo
- Generación auto-regresiva token por token
- Post-entrenamiento: fine-tuning para tareas específicas
- Escala masiva en tres dimensiones: parámetros, datos, cómputo

## 🔗 Connections
- [[Redes Neuronales]]
- [[Mecanismo de Atención]]
- [[Ventana Contextual]]
- [[Tokenización]]
- [[Embeddings]]

## 💡 Personal Insight
Los LLMs representan una emergencia de capacidades generales a partir del simple objetivo de predicción de siguiente palabra, demostrando que la escala puede generar inteligencia.

## 🧾 Recursos
-