---
Fecha de creaci√≥n: <% tp.date.now("YYYY-MM-DD HH:mm") %>
Fecha de Modificaci√≥n: <% tp.date.now("YYYY-MM-DD HH:mm") %>
tags: #tokenization #preprocessing #nlp #bpe #subword #vocabulary
Tema: Tokenizaci√≥n
---

## üìö Idea/Concepto 
La tokenizaci√≥n es el proceso de preprocesamiento que segmenta texto crudo en unidades at√≥micas (tokens) y las mapea a identificadores num√©ricos √∫nicos que el modelo puede procesar. Utiliza algoritmos de tokenizaci√≥n subword como Byte-Pair Encoding (BPE), WordPiece o SentencePiece para balancear granularidad y eficiencia del vocabulario. Maneja palabras fuera del vocabulario (OOV) descomponi√©ndolas en subpalabras conocidas. Incluye tokens especiales para funcionalidades espec√≠ficas: [CLS] (clasificaci√≥n), [SEP] (separaci√≥n), <EOS> (fin de secuencia), <PAD> (relleno), <UNK> (desconocido), <BOS> (inicio). Los IDs resultantes sirven como √≠ndices directos para la matriz de embeddings, determinando la representaci√≥n inicial y la eficiencia computacional del procesamiento secuencial.

## üìå Puntos Claves
- Pipeline: Texto crudo ‚Üí Normalizaci√≥n ‚Üí Segmentaci√≥n ‚Üí Tokens ‚Üí IDs num√©ricos
- Algoritmos principales:
  - SentencePiece: Trata texto como secuencia de Unicode, incluye espacios
- Manejo de vocabulario: Tama√±o fijo (t√≠pico: 30K-50K tokens)
- Impacto downstream: Determina l√≠mites de la ventana contextual efectiva
- Reversibilidad: Detokenizaci√≥n debe preservar texto original

## üîó Connections
- [[Embeddings]]
- [[Large Language Models]]
- [[Ventana Contextual]]
- [[Redes Neuronales]]
- [[Preprocessing de Texto]]
- [[Vocabulario y OOV]]

## üí° Personal Insight
La tokenizaci√≥n es el primer cuello de botella informacional del pipeline de NLP. Su dise√±o determina qu√© granularidad ling√º√≠stica puede capturar el modelo y c√≥mo de eficientemente puede procesarla. Es el arte de encontrar las "unidades naturales" del lenguaje que optimicen tanto la expresividad como la computaci√≥n.

## üßæ Recursos