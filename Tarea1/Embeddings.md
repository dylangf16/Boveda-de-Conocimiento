---
Fecha de creaci√≥n: <% tp.date.now("YYYY-MM-DD HH:mm") %>
Fecha de Modificaci√≥n: <% tp.date.now("YYYY-MM-DD HH:mm") %>
tags:
Tema: Embeddings
---

## üìö Idea/Concepto 
Los embeddings son representaciones vectoriales continuas que convierten tokens discretos (IDs num√©ricos) en vectores de alta dimensionalidad mediante una matriz de lookup entrenable. Cada fila representa un token del vocabulario y cada columna una dimensi√≥n latente que captura caracter√≠sticas sem√°nticas, sint√°cticas y contextuales. Los vectores resultantes permiten operaciones aritm√©ticas que reflejan relaciones ling√º√≠sticas an√°logas (ej: rey - hombre + mujer ‚âà reina). Se combinan con codificaciones posicionales para preservar informaci√≥n de orden secuencial. En Transformers, la matriz de embeddings se comparte t√≠picamente con la transformaci√≥n lineal final (tied weights) para eficiencia de par√°metros, y se aplica escalado por ‚àöd_model para estabilizar gradientes durante entrenamiento.

## üìå Puntos Claves
- Arquitectura: Matriz de lookup entrenable (vocab_size √ó embedding_dim)
- Dimensiones capturadas: sem√°ntica, sintaxis, morfolog√≠a, contexto
- Propiedades geom√©tricas: distancia coseno refleja similaridad sem√°ntica
- Integraci√≥n posicional: suma con encodings posicionales en Transformers
- Inicializaci√≥n: t√≠picamente aleatoria (distribuci√≥n normal peque√±a)

## üîó Connections
- [[Tokenizaci√≥n]]
- [[Large Language Models]]
- [[Redes Neuronales]]
- [[Mecanismo de Atenci√≥n]]
- [[Ventana Contextual]]
- [[Codificaci√≥n Posicional]]

## üí° Personal Insight
Los embeddings son el puente fundamental entre s√≠mbolos discretos y matem√°ticas continuas, convirtiendo el lenguaje en geometr√≠a donde la distancia y direcci√≥n tienen significado sem√°ntico. Su calidad determina la capacidad representacional de todo el modelo.

## üßæ Recursos